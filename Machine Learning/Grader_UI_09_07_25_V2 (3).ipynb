{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4086b59-5e7e-4398-9038-61d925062876",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install gradio>=4.36.0 pandas reportlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e9a5449-bfd8-481b-a6f8-abb7196ba866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io, json, traceback, datetime, re\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "\n",
    "from openai import OpenAI\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# --- PDF helpers (ReportLab) ---\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib import colors\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, PageBreak, Preformatted\n",
    "\n",
    "from xml.sax.saxutils import escape  # for safe Paragraph content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37db8f8c-c895-4ad0-8378-69a1817ff413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate generation\n",
    "def generate_candidate_answer(task_prompt: str, *, candidate_model: str = \"gpt-4o-mini\") -> str:\n",
    "\n",
    "    # Generate a sample response to prompt with the model-under-test.\n",
    "\n",
    "    vs = globals().get(\"vs_id\")\n",
    "\n",
    "    # Build tools only when a vector store is available\n",
    "    tools = [{\"type\": \"file_search\", \"vector_store_ids\": [vs]}] if vs else None\n",
    "\n",
    "    # Only include 'tools' in the API call if we actually built it\n",
    "    extra_kwargs = {\"tools\": tools} if tools else {}\n",
    "\n",
    "    resp = client.responses.create(\n",
    "        model=candidate_model,\n",
    "        input=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"Follow the instructions carefully. Use attached files when noted. Include any code, values, files, or plots the task requires.\\n\\n\"\n",
    "                f\"TASK:\\n{task_prompt}\"\n",
    "            )\n",
    "        }],\n",
    "        **({\"tools\": tools} if tools else {})\n",
    "        #tools=[{\"type\": \"file_search\", \"vector_store_ids\": [vs_id]}],\n",
    "        # temperature=0\n",
    "        #**extra_kwargs\n",
    "    )\n",
    "    # SDK compatibility: try the modern accessor, fall back if needed.\n",
    "    try:\n",
    "        return resp.output_text\n",
    "    except AttributeError:\n",
    "        return resp.output[0].content[0].text\n",
    "\n",
    "# Rubric preparation (ONLY criterion + score)\n",
    "def _prepare_rubric_items(raw_items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Prepare rubric items for grading. IMPORTANT: penalty is determined ONLY by the sign of `score`.\n",
    "      - score > 0  => reward item\n",
    "      - score < 0  => penalty item\n",
    "    \"\"\"\n",
    "    prepped = []\n",
    "    for it in raw_items:\n",
    "        crit  = (it.get(\"criterion\") or \"\").strip()\n",
    "        score = float(it.get(\"score\", 0))\n",
    "        rid   = it.get(\"rubricItemId\") or \"\"\n",
    "        is_penalty = (score < 0)  # <-- CHANGED: sign-of-score, not wording\n",
    "        prepped.append({\n",
    "            \"rubricItemId\": rid,\n",
    "            \"criterion\": crit,\n",
    "            \"score\": score,          # positive or negative; used as-is\n",
    "            \"is_penalty\": is_penalty\n",
    "        })\n",
    "    return prepped\n",
    "\n",
    "def _build_rubric_prompts(sample, completion, items):\n",
    "    # Only pass what the grader truly needs: id + criterion (hide score/is_penalty)\n",
    "    compact = [{\n",
    "        \"rubricItemId\": it[\"rubricItemId\"],\n",
    "        \"criterion\": it[\"criterion\"]\n",
    "    } for it in items]\n",
    "\n",
    "    system_message = (\n",
    "        \"You are a meticulous grader that outputs STRICT JSON per schema.\\n\"\n",
    "        \"Evaluate each rubric item independently using ONLY its 'criterion' text. \"\n",
    "        \"Ignore all other rubric metadata (scores, penalties, tags, sources) and any external facts.\\n\"\n",
    "        \"Unit handling: If a numeric value in the candidate is within the stated tolerance after proper unit conversion, \"\n",
    "        \"consider the criterion satisfied even if the candidate does not restate the value in the rubric's unit.\\n\"\n",
    "        \"Consistency rule: The boolean 'met' must reflect the final verdict in your rationale.\\n\"\n",
    "    )\n",
    "\n",
    "    user_message = f\"\"\"\n",
    "TASK PROMPT:\n",
    "{sample.get('prompt','')}\n",
    "\n",
    "CANDIDATE ANSWER:\n",
    "{completion}\n",
    "\n",
    "RUBRIC_ITEMS (evaluate ONLY against criterion; ignore any other metadata):\n",
    "{json.dumps(compact, ensure_ascii=False)}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Decide each item strictly from the candidate answer vs. the item's 'criterion'. Do NOT use outside knowledge.\n",
    "- Treat values as equivalent across unit systems if, after correct conversion, the value is within the stated tolerance.\n",
    "- Write a short rationale. The last sentence MUST be exactly one of:\n",
    "  \"Verdict: MET\"  or  \"Verdict: NOT MET\".\n",
    "- Set the boolean 'met' to True iff the final sentence is \"Verdict: MET\"; else set it to False.\n",
    "- Never output a rationale that concludes \"Verdict: MET\" while setting met=false, or vice versa.\n",
    "- If a tolerance is written in a criteria and the value in the response is within the tolerance, then \"Verdict: MET\"\n",
    "\n",
    "OUTPUT (STRICT JSON ONLY):\n",
    "{{\n",
    "  \"items\": [\n",
    "    {{\n",
    "      \"rubricItemId\": \"string\",\n",
    "      \"met\": true | false,\n",
    "      \"rationale\": \"string that ends with 'Verdict: MET' or 'Verdict: NOT MET'\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "    return system_message, user_message\n",
    "\n",
    "\n",
    "def _rubric_bounds(items: List[Dict[str, Any]]) -> Tuple[float, float]:\n",
    "    pos = sum(s[\"score\"] for s in items if s[\"score\"] > 0)\n",
    "    neg = sum(s[\"score\"] for s in items if s[\"score\"] < 0)\n",
    "    return pos, neg  # (max positive, min negative)\n",
    "\n",
    "\n",
    "# Exact-match grader (optional)\n",
    "\n",
    "def simple_string_grader(sample, completion) -> float:\n",
    "\n",
    "    #1.0 if completion exactly matches sample['reference'], else 0.0.\n",
    "    #Leave sample['reference'] empty ('') to effectively ignore this.\n",
    "\n",
    "    ref = (sample.get(\"reference\") or \"\").strip()\n",
    "    if not ref:\n",
    "        return 0.0\n",
    "    return 1.0 if completion.strip() == ref else 0.0\n",
    "\n",
    "\n",
    "def model_grader_with_rubric(sample, completion, raw_rubric_items, *, grader_model: str = \"gpt-4o-mini\"):\n",
    "    items = _prepare_rubric_items(raw_rubric_items)\n",
    "    pos_max = sum(max(0.0, float(it[\"score\"])) for it in items)\n",
    "    neg_min = sum(min(0.0, float(it[\"score\"])) for it in items)\n",
    "    bounds = {\"pos_max\": float(pos_max), \"neg_min\": float(neg_min)}\n",
    "\n",
    "    system_message, user_message = _build_rubric_prompts(sample, completion, items)\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=grader_model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "        ],\n",
    "        #temperature=0,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "\n",
    "    content = resp.choices[0].message.content\n",
    "\n",
    "    def _strip_fences(s: str) -> str:\n",
    "        s = s.strip()\n",
    "        if s.startswith(\"```\"):\n",
    "            parts = s.split(\"```\")\n",
    "            if len(parts) >= 3:\n",
    "                s = parts[1]\n",
    "            s = s.lstrip()\n",
    "            if s.startswith(\"json\"):\n",
    "                s = s[4:].lstrip()\n",
    "        return s\n",
    "\n",
    "    try:\n",
    "        rubric_json = json.loads(content)\n",
    "    except Exception:\n",
    "        rubric_json = json.loads(_strip_fences(content))\n",
    "\n",
    "    if not isinstance(rubric_json, dict) or \"items\" not in rubric_json:\n",
    "        raise ValueError(\"Grader did not return JSON with an 'items' array per schema.\")\n",
    "\n",
    "        # Enforce: boolean must match final \"Verdict: ...\" line in rationale\n",
    "    for it in rubric_json.get(\"items\", []):\n",
    "        tail = re.sub(r\"\\s+\", \" \", str(it.get(\"rationale\", \"\"))).strip().lower().rstrip(\" .!;\")\n",
    "    if tail.endswith(\"verdict: met\"):\n",
    "        it[\"met\"] = True\n",
    "    elif tail.endswith(\"verdict: not met\"):\n",
    "        it[\"met\"] = False\n",
    "\n",
    "    return rubric_json, bounds\n",
    "\n",
    "\n",
    "def _compute_signed_total_from_decisions(rubric_json: dict, raw_rubric_items):\n",
    "    \"\"\"\n",
    "    Compute signed total strictly from rubric item scores and decisions.\n",
    "    Also return enriched per-item rows including:\n",
    "      - criterion_number (1-indexed position in the original rubric JSON)\n",
    "      - point_value (the rubric score, positive or negative)\n",
    "      - awarded (0 or point_value, unless a valid 'awarded' was provided)\n",
    "\n",
    "    Rules:\n",
    "    - If 'met' is True -> contribution = rubric score (can be negative for penalties)\n",
    "    - If 'met' is False -> contribution = 0\n",
    "    - Only trust 'awarded' if it is exactly 0 or exactly equal to the rubric score\n",
    "    \"\"\"\n",
    "    # Build lookups from the rubric\u2019s original order\n",
    "    id_to_score = {}\n",
    "    id_to_index1 = {}\n",
    "    id_to_criterion = {}\n",
    "    for idx, it in enumerate(raw_rubric_items, start=1):\n",
    "        rid = it.get(\"rubricItemId\") or it.get(\"id\") or (it.get(\"criterion\", \"\")[:48])\n",
    "        sc = float(it.get(\"score\", 0.0))\n",
    "        id_to_score[rid] = sc\n",
    "        id_to_index1[rid] = idx\n",
    "        id_to_criterion[rid] = it.get(\"criterion\", \"\")\n",
    "\n",
    "    # Pull decisions list from the grader output (be tolerant of formats)\n",
    "    if isinstance(rubric_json, dict):\n",
    "        decisions = rubric_json.get(\"items\") or rubric_json.get(\"results\") or []\n",
    "    elif isinstance(rubric_json, list):\n",
    "        decisions = rubric_json\n",
    "    else:\n",
    "        decisions = []\n",
    "\n",
    "    items_out = []\n",
    "    total = 0.0\n",
    "\n",
    "    for dec in decisions:\n",
    "        rid = dec.get(\"rubricItemId\") or dec.get(\"id\") or (dec.get(\"criterion\", \"\")[:48])\n",
    "        score = id_to_score.get(rid, 0.0)\n",
    "        met = bool(dec.get(\"met\", False))\n",
    "\n",
    "        # Only trust 'awarded' if it's exactly 0 or exactly equal to the rubric score\n",
    "        aw = dec.get(\"awarded\", None)\n",
    "        if isinstance(aw, (int, float)) and (abs(float(aw) - score) < 1e-9 or abs(float(aw)) < 1e-9):\n",
    "            awarded = float(aw)\n",
    "        else:\n",
    "            awarded = score if met else 0.0\n",
    "\n",
    "        total += awarded\n",
    "        items_out.append({\n",
    "            \"criterion_number\": id_to_index1.get(rid, None),\n",
    "            \"rubricItemId\": rid,\n",
    "            \"criterion\": id_to_criterion.get(rid, \"\"),\n",
    "            \"point_value\": score,\n",
    "            \"met\": met,\n",
    "            \"awarded\": awarded,\n",
    "            \"rationale\": dec.get(\"rationale\", \"\")\n",
    "        })\n",
    "\n",
    "    return total, items_out\n",
    "\n",
    "# Normalize rubric total to [0,1] and blend with exact-match\n",
    "def _normalize_total_score(total: float, pos_max: float, neg_min: float) -> float:\n",
    "    \"\"\"\n",
    "    Map a signed total into [0,1] given bounds:\n",
    "      neg_min <= total <= pos_max\n",
    "    \"\"\"\n",
    "    lo = float(neg_min)\n",
    "    hi = float(pos_max)\n",
    "    span = hi - lo\n",
    "    if span == 0:\n",
    "        return 0.5\n",
    "    norm = (float(total) - lo) / span\n",
    "    # Clamp for safety\n",
    "    return max(0.0, min(1.0, norm))\n",
    "\n",
    "\n",
    "def multigrader(sample, completion, raw_rubric_items,\n",
    "                *, grader_model: str = \"gpt-4o-mini\",\n",
    "                exact_weight: float = 0.0, rubric_weight: float = 1.0):\n",
    "    \"\"\"\n",
    "    Returns (final_score_0_1, details).\n",
    "    This version recomputes score from decisions.\n",
    "    - Penalties/rewards are determined solely by the sign of each rubric item's score.\n",
    "    - 'rubric_details' returns enriched per-item rows (criterion number, point value, rationale, etc.).\n",
    "    \"\"\"\n",
    "    # 1) Exact-match channel (if you use it)\n",
    "    exact = simple_string_grader(sample, completion)\n",
    "\n",
    "    # 2) Ask the grader model for per-item decisions and bounds\n",
    "    #    Expect: rubric_json = {\"items\":[...]} and bounds = {\"pos_max\": float, \"neg_min\": float}\n",
    "    rubric_json, bounds = model_grader_with_rubric(\n",
    "        sample, completion, raw_rubric_items, grader_model=grader_model\n",
    "    )\n",
    "\n",
    "    # 3) Recompute signed total from decisions (now returns (signed_total, items_out))\n",
    "    signed_total, items_out = _compute_signed_total_from_decisions(rubric_json, raw_rubric_items)\n",
    "\n",
    "    # 4) Normalize and blend\n",
    "    rubric_norm = _normalize_total_score(signed_total, bounds.get(\"pos_max\", 0.0), bounds.get(\"neg_min\", 0.0))\n",
    "    final = (exact_weight * float(exact)) + (rubric_weight * float(rubric_norm))\n",
    "\n",
    "    # 5) Return details\n",
    "    return final, {\n",
    "        \"exact_match\": float(exact),\n",
    "        \"rubric_total\": float(signed_total),      # signed\n",
    "        \"rubric_norm\": float(rubric_norm),\n",
    "        \"rubric_bounds\": bounds,\n",
    "        \"rubric_details\": {\"items\": items_out},   # enriched rows for your table/JSON panel\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bd76d88-206a-4e32-b984-62119bb57b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final state grading UI helpers and pipelines\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _fpath(obj):\n",
    "    if obj is None:\n",
    "        return None\n",
    "    return getattr(obj, \"name\", None) or getattr(obj, \"path\", None) or str(obj)\n",
    "\n",
    "\n",
    "def _read_text_file(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def _read_json_file(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def _ensure_client_from_key(api_key_text):\n",
    "    if \"client\" in globals() and globals()[\"client\"] is not None:\n",
    "        return globals()[\"client\"]\n",
    "    api_key = (api_key_text or \"\").strip() or os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"No OpenAI client found and no API key provided.\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"Please install openai>=1.40.0\") from e\n",
    "    globals()[\"client\"] = OpenAI(api_key=api_key)\n",
    "    return globals()[\"client\"]\n",
    "\n",
    "\n",
    "class _LooseJSONParser:\n",
    "    \"\"\"Parse the lightly formatted \"final state\" export into Python data.\"\"\"\n",
    "\n",
    "    __slots__ = (\"s\", \"n\", \"i\")\n",
    "\n",
    "    def __init__(self, text: str):\n",
    "        self.s = text\n",
    "        self.n = len(text)\n",
    "        self.i = 0\n",
    "\n",
    "    def _error(self, msg: str):\n",
    "        context = self.s[self.i:self.i + 60]\n",
    "        raise ValueError(f\"{msg} at pos {self.i}: {context!r}\")\n",
    "\n",
    "    def _skip_ws(self):\n",
    "        while self.i < self.n and self.s[self.i] in \" \\t\\r\\n\":\n",
    "            self.i += 1\n",
    "\n",
    "    def parse(self):\n",
    "        self._skip_ws()\n",
    "        value = self._parse_value(relaxed=True)\n",
    "        self._skip_ws()\n",
    "        return value\n",
    "\n",
    "    def _parse_value(self, relaxed: bool = False):\n",
    "        self._skip_ws()\n",
    "        if self.i >= self.n:\n",
    "            self._error(\"Unexpected end of input\")\n",
    "        ch = self.s[self.i]\n",
    "        if ch == '\"':\n",
    "            return self._parse_string(relaxed=relaxed)\n",
    "        if ch == '{':\n",
    "            return self._parse_object()\n",
    "        if ch == '[':\n",
    "            return self._parse_array()\n",
    "        if self.s.startswith(\"true\", self.i):\n",
    "            self.i += 4\n",
    "            return True\n",
    "        if self.s.startswith(\"false\", self.i):\n",
    "            self.i += 5\n",
    "            return False\n",
    "        if self.s.startswith(\"null\", self.i):\n",
    "            self.i += 4\n",
    "            return None\n",
    "        start = self.i\n",
    "        while self.i < self.n and self.s[self.i] not in \" \\t\\r\\n,]}\":\n",
    "            self.i += 1\n",
    "        token = self.s[start:self.i].strip()\n",
    "        if not token:\n",
    "            self._error(\"Empty token\")\n",
    "        try:\n",
    "            if any(c in token for c in (\".\", \"e\", \"E\")):\n",
    "                return float(token)\n",
    "            return int(token)\n",
    "        except ValueError:\n",
    "            return token\n",
    "\n",
    "    def _parse_string(self, relaxed: bool = False) -> str:\n",
    "        if self.s[self.i] != '\"':\n",
    "            self._error(\"Expected string start\")\n",
    "        self.i += 1\n",
    "        chars = []\n",
    "        while self.i < self.n:\n",
    "            ch = self.s[self.i]\n",
    "            if ch == '\\\\':\n",
    "                if self.i + 1 >= self.n:\n",
    "                    self._error(\"Bad escape sequence\")\n",
    "                nxt = self.s[self.i + 1]\n",
    "                esc_map = {'\"': '\"', '\\\\': '\\\\', '/': '/', 'b': '\\b', 'f': '\\f', 'n': '\\n', 'r': '\\r', 't': '\\t'}\n",
    "                if nxt in esc_map:\n",
    "                    chars.append(esc_map[nxt])\n",
    "                    self.i += 2\n",
    "                    continue\n",
    "                if nxt == 'u':\n",
    "                    hex_digits = self.s[self.i + 2:self.i + 6]\n",
    "                    chars.append(chr(int(hex_digits, 16)))\n",
    "                    self.i += 6\n",
    "                    continue\n",
    "                chars.append(nxt)\n",
    "                self.i += 2\n",
    "                continue\n",
    "            if ch == '\"':\n",
    "                nxt = self.s[self.i + 1] if self.i + 1 < self.n else ''\n",
    "                if not relaxed or nxt in ('', '\\n', '\\r', ',', '}', ']'):\n",
    "                    self.i += 1\n",
    "                    return ''.join(chars)\n",
    "                chars.append(ch)\n",
    "                self.i += 1\n",
    "                continue\n",
    "            chars.append(ch)\n",
    "            self.i += 1\n",
    "        self._error(\"Unterminated string\")\n",
    "\n",
    "    def _parse_object(self):\n",
    "        if self.s[self.i] != '{':\n",
    "            self._error(\"Expected '{'\")\n",
    "        self.i += 1\n",
    "        obj = {}\n",
    "        while True:\n",
    "            self._skip_ws()\n",
    "            if self.i < self.n and self.s[self.i] == '}':\n",
    "                self.i += 1\n",
    "                return obj\n",
    "            key = self._parse_string(relaxed=False)\n",
    "            self._skip_ws()\n",
    "            if self.i >= self.n or self.s[self.i] != ':':\n",
    "                self._error(\"Expected ':' after key\")\n",
    "            self.i += 1\n",
    "            value = self._parse_value(relaxed=True)\n",
    "            obj[key] = value\n",
    "            self._skip_ws()\n",
    "            if self.i < self.n and self.s[self.i] == ',':\n",
    "                self.i += 1\n",
    "                continue\n",
    "            self._skip_ws()\n",
    "            if self.i < self.n and self.s[self.i] == '}':\n",
    "                continue\n",
    "\n",
    "    def _parse_array(self):\n",
    "        if self.s[self.i] != '[':\n",
    "            self._error(\"Expected '['\")\n",
    "        self.i += 1\n",
    "        arr = []\n",
    "        while True:\n",
    "            self._skip_ws()\n",
    "            if self.i < self.n and self.s[self.i] == ']':\n",
    "                self.i += 1\n",
    "                return arr\n",
    "            start = self.i\n",
    "            while self.i < self.n and self.s[self.i].isdigit():\n",
    "                self.i += 1\n",
    "            if self.i > start and self.i < self.n and self.s[self.i] == ':':\n",
    "                self.i += 1\n",
    "            value = self._parse_value(relaxed=True)\n",
    "            arr.append(value)\n",
    "            self._skip_ws()\n",
    "            if self.i < self.n and self.s[self.i] == ',':\n",
    "                self.i += 1\n",
    "                continue\n",
    "\n",
    "\n",
    "def _load_final_state(path) -> Dict[str, Any]:\n",
    "    raw = Path(path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    text = raw.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\").replace(\"NULL\", \"null\")\n",
    "    parser = _LooseJSONParser(text)\n",
    "    return parser.parse()\n",
    "\n",
    "\n",
    "def _iter_dicts(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        yield obj\n",
    "        for value in obj.values():\n",
    "            yield from _iter_dicts(value)\n",
    "    elif isinstance(obj, list):\n",
    "        for item in obj:\n",
    "            yield from _iter_dicts(item)\n",
    "\n",
    "\n",
    "def _find_first_str(obj, keys):\n",
    "    lowered = [k.lower() for k in keys]\n",
    "    for mapping in _iter_dicts(obj):\n",
    "        for key, value in mapping.items():\n",
    "            if key.lower() in lowered and isinstance(value, str) and value.strip():\n",
    "                return value.strip()\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def _extract_prompt_from_state(state):\n",
    "    return _find_first_str(state, [\"prompt\", \"task\", \"instructions\", \"question\", \"request\"])\n",
    "\n",
    "\n",
    "def _extract_candidate_answer(state):\n",
    "    direct = _find_first_str(state, [\n",
    "        \"final_response\", \"final_output\", \"response\", \"answer\", \"output_text\", \"completion\"\n",
    "    ])\n",
    "    if direct:\n",
    "        return direct\n",
    "    messages = state.get(\"messages\") if isinstance(state, dict) else None\n",
    "    if isinstance(messages, list):\n",
    "        for msg in reversed(messages):\n",
    "            if not isinstance(msg, dict):\n",
    "                continue\n",
    "            sender = str(msg.get(\"from\", \"\")).lower()\n",
    "            if sender.startswith(\"you\") or \"you@\" in sender:\n",
    "                text = msg.get(\"text\") or msg.get(\"body\") or msg.get(\"content\")\n",
    "                if isinstance(text, str) and text.strip():\n",
    "                    return text.strip()\n",
    "                html = msg.get(\"html\")\n",
    "                if isinstance(html, str) and html.strip():\n",
    "                    return html.strip()\n",
    "    fallback = _find_first_str(state, [\"text\", \"content\", \"message\"])\n",
    "    if fallback:\n",
    "        return fallback\n",
    "    raise ValueError(\"Could not locate a candidate response inside the final state file.\")\n",
    "\n",
    "\n",
    "def _build_items_table(items_raw):\n",
    "    rows = []\n",
    "    for idx, it in enumerate(items_raw or [], start=1):\n",
    "        criterion_number = it.get(\"criterion_number\", idx)\n",
    "        criterion = it.get(\"criterion\") or it.get(\"rubricItemId\") or \"\"\n",
    "        point_value = it.get(\"point_value\", it.get(\"score\", 0.0))\n",
    "        try:\n",
    "            point_value = float(point_value)\n",
    "        except Exception:\n",
    "            point_value = 0.0\n",
    "        awarded = it.get(\"awarded\", point_value if it.get(\"met\") else 0.0)\n",
    "        try:\n",
    "            awarded = float(awarded)\n",
    "        except Exception:\n",
    "            awarded = 0.0\n",
    "        rationale = it.get(\"rationale\", \"\")\n",
    "        rows.append({\n",
    "            \"#\": criterion_number,\n",
    "            \"criterion\": criterion,\n",
    "            \"possible_score\": point_value,\n",
    "            \"score_awarded\": awarded,\n",
    "            \"explanation\": rationale,\n",
    "        })\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=[\"#\", \"criterion\", \"possible_score\", \"score_awarded\", \"explanation\"])\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df[\"#\"].notna().any():\n",
    "        df = df.sort_values(by=[\"#\"], kind=\"stable\")\n",
    "    return df[[\"#\", \"criterion\", \"possible_score\", \"score_awarded\", \"explanation\"]]\n",
    "\n",
    "\n",
    "def _out_dir():\n",
    "    env = os.getenv(\"GRADING_REPORT_DIR\", \"\")\n",
    "    if env:\n",
    "        base = Path(env).expanduser()\n",
    "    else:\n",
    "        candidates = [Path.home() / \"Downloads\", Path.home() / \"OneDrive\" / \"Downloads\"]\n",
    "        base = next((p for p in candidates if p.exists()), Path.cwd() / \"grading_reports\")\n",
    "    base.mkdir(parents=True, exist_ok=True)\n",
    "    return base\n",
    "\n",
    "\n",
    "def _make_pdf(output_path, context):\n",
    "    output_path = Path(output_path)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    styles = getSampleStyleSheet()\n",
    "    styles.add(ParagraphStyle(\n",
    "        name=\"Cell\",\n",
    "        parent=styles[\"Normal\"],\n",
    "        fontSize=9,\n",
    "        leading=11,\n",
    "        splitLongWords=True,\n",
    "    ))\n",
    "    mono = ParagraphStyle(\n",
    "        name=\"Mono\",\n",
    "        parent=styles[\"Normal\"],\n",
    "        fontName=\"Courier\",\n",
    "        fontSize=9,\n",
    "        leading=10,\n",
    "    )\n",
    "\n",
    "    doc = SimpleDocTemplate(\n",
    "        str(output_path),\n",
    "        pagesize=letter,\n",
    "        rightMargin=36,\n",
    "        leftMargin=36,\n",
    "        topMargin=36,\n",
    "        bottomMargin=36,\n",
    "    )\n",
    "\n",
    "    story = []\n",
    "    story.append(Paragraph(\"Grading Report\", styles[\"Title\"]))\n",
    "    story.append(Spacer(1, 6))\n",
    "    meta_lines = [\n",
    "        f\"Generated: {context['timestamp']}\",\n",
    "        f\"Submission file: {context.get('submission_name') or '\u2014'}\",\n",
    "        f\"Grader model: {context['grader_model']}\",\n",
    "        f\"Prompt file: {context.get('prompt_name') or '\u2014'}\",\n",
    "        f\"Rubric file: {context.get('rubric_name') or '\u2014'}\",\n",
    "    ]\n",
    "    story.append(Paragraph(\"<br/>\".join(meta_lines), styles[\"Normal\"]))\n",
    "    story.append(Spacer(1, 12))\n",
    "\n",
    "    story.append(Paragraph(\"Candidate Response\", styles[\"Heading2\"]))\n",
    "    story.append(Spacer(1, 4))\n",
    "    story.append(Preformatted(context.get(\"candidate_answer\") or \"\u2014\", mono))\n",
    "    story.append(Spacer(1, 12))\n",
    "\n",
    "    story.append(Paragraph(\"Per-Criterion Scores\", styles[\"Heading2\"]))\n",
    "    story.append(Spacer(1, 6))\n",
    "\n",
    "    df = context[\"items_df\"].copy()\n",
    "\n",
    "    def P(x):\n",
    "        return Paragraph(escape(str(x)).replace(\"\\n\", \"<br/>\"), styles[\"Cell\"])\n",
    "\n",
    "    headers = [\"Criterion\", \"Score Awarded\", \"Explanation\"]\n",
    "    table_data = [headers]\n",
    "    for _, row in df.iterrows():\n",
    "        table_data.append([\n",
    "            P(row[\"criterion\"]),\n",
    "            f\"{float(row['score_awarded']):g}\",\n",
    "            P(row[\"explanation\"]),\n",
    "        ])\n",
    "\n",
    "    col_widths = [doc.width * f for f in (0.35, 0.15, 0.50)]\n",
    "    tbl = Table(table_data, colWidths=col_widths, repeatRows=1)\n",
    "    tbl.setStyle(TableStyle([\n",
    "        (\"FONT\", (0, 0), (-1, 0), \"Helvetica-Bold\", 10),\n",
    "        (\"BACKGROUND\", (0, 0), (-1, 0), colors.lightgrey),\n",
    "        (\"LINEABOVE\", (0, 0), (-1, 0), 0.5, colors.black),\n",
    "        (\"LINEBELOW\", (0, 0), (-1, 0), 0.5, colors.black),\n",
    "        (\"GRID\", (0, 1), (-1, -1), 0.25, colors.grey),\n",
    "        (\"VALIGN\", (0, 0), (-1, -1), \"TOP\"),\n",
    "        (\"LEFTPADDING\", (0, 0), (-1, -1), 4),\n",
    "        (\"RIGHTPADDING\", (0, 0), (-1, -1), 4),\n",
    "        (\"WORDWRAP\", (0, 0), (-1, -1), True),\n",
    "    ]))\n",
    "    story.append(tbl)\n",
    "    story.append(Spacer(1, 12))\n",
    "\n",
    "    signed = float(context[\"signed_total\"])\n",
    "    norm = float(context[\"normalized\"])\n",
    "    story.append(Paragraph(\"Totals\", styles[\"Heading2\"]))\n",
    "    story.append(Paragraph(f\"Total (signed): <b>{signed:g}</b>\", styles[\"Normal\"]))\n",
    "    story.append(Paragraph(f\"Normalized (0..1): <b>{norm:.3f}</b>\", styles[\"Normal\"]))\n",
    "\n",
    "    doc.build(story)\n",
    "    return str(output_path)\n",
    "\n",
    "\n",
    "def run_pipeline(prompt_file, rubric_file, final_state_file,\n",
    "                 grader_model, api_key_text, exact_weight, rubric_weight):\n",
    "    logs = []\n",
    "\n",
    "    def log(msg):\n",
    "        logs.append(str(msg))\n",
    "\n",
    "    try:\n",
    "        cli = _ensure_client_from_key(api_key_text)\n",
    "        log(\"OpenAI client ready.\")\n",
    "\n",
    "        if not rubric_file or not final_state_file:\n",
    "            return (\n",
    "                None,\n",
    "                None,\n",
    "                \"\",\n",
    "                pd.DataFrame(),\n",
    "                None,\n",
    "                \"\\n\".join(logs + [\"Please upload both a rubric JSON file and a final state JSON file.\"]),\n",
    "                \"\",\n",
    "                \"\",\n",
    "                \"\",\n",
    "                \"\",\n",
    "            )\n",
    "\n",
    "        rubric_path = _fpath(rubric_file)\n",
    "        rubric_raw = _read_json_file(rubric_path)\n",
    "        if not isinstance(rubric_raw, list) or not all((\"criterion\" in x and \"score\" in x) for x in rubric_raw):\n",
    "            return (\n",
    "                None,\n",
    "                None,\n",
    "                \"\",\n",
    "                pd.DataFrame(),\n",
    "                None,\n",
    "                \"\\n\".join(logs + [\"Rubric must be a JSON list of {criterion, score} objects.\"]),\n",
    "                \"\",\n",
    "                \"\",\n",
    "                \"\",\n",
    "                \"\",\n",
    "            )\n",
    "\n",
    "        prompt_path = _fpath(prompt_file) if prompt_file else None\n",
    "        prompt_text = _read_text_file(prompt_path) if prompt_path else \"\"\n",
    "        prompt_name = Path(prompt_path).name if prompt_path else \"\"\n",
    "\n",
    "        submission_path = _fpath(final_state_file)\n",
    "        submission_name = Path(submission_path).name if submission_path else \"\"\n",
    "        final_state = _load_final_state(submission_path)\n",
    "        log(f\"Loaded final state: {submission_name}\")\n",
    "\n",
    "        candidate_answer = _extract_candidate_answer(final_state)\n",
    "        log(\"Candidate response extracted from final state.\")\n",
    "\n",
    "        if not prompt_text:\n",
    "            derived_prompt = _extract_prompt_from_state(final_state)\n",
    "            if derived_prompt:\n",
    "                prompt_text = derived_prompt\n",
    "                log(\"Derived prompt from final state.\")\n",
    "\n",
    "        rubric_name = Path(rubric_path).name if rubric_path else \"\"\n",
    "        log(f\"Loaded rubric with {len(rubric_raw)} items.\")\n",
    "\n",
    "        sample = {\"prompt\": prompt_text, \"reference\": \"\"}\n",
    "        final_norm, details = multigrader(\n",
    "            sample,\n",
    "            candidate_answer,\n",
    "            rubric_raw,\n",
    "            grader_model=grader_model,\n",
    "            exact_weight=float(exact_weight),\n",
    "            rubric_weight=float(rubric_weight),\n",
    "        )\n",
    "        log(\"Grading complete.\")\n",
    "\n",
    "        items_raw = (details or {}).get(\"rubric_details\", {}).get(\"items\", [])\n",
    "        items_df = _build_items_table(items_raw)\n",
    "        signed_total = float((details or {}).get(\"rubric_total\", 0.0))\n",
    "        final_norm = float(final_norm)\n",
    "\n",
    "        out_dir = _out_dir()\n",
    "        ts = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        pdf_path = out_dir / f\"grading_report_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf\"\n",
    "        _make_pdf(pdf_path, {\n",
    "            \"timestamp\": ts,\n",
    "            \"submission_name\": submission_name,\n",
    "            \"grader_model\": grader_model,\n",
    "            \"prompt_name\": prompt_name,\n",
    "            \"rubric_name\": rubric_name,\n",
    "            \"candidate_answer\": candidate_answer,\n",
    "            \"items_df\": items_df,\n",
    "            \"signed_total\": signed_total,\n",
    "            \"normalized\": final_norm,\n",
    "        })\n",
    "        log(f\"PDF created: {pdf_path}\")\n",
    "\n",
    "        return (\n",
    "            final_norm,\n",
    "            signed_total,\n",
    "            candidate_answer,\n",
    "            items_df,\n",
    "            str(pdf_path),\n",
    "            \"\\n\".join(logs),\n",
    "            prompt_text,\n",
    "            prompt_name,\n",
    "            submission_name,\n",
    "            candidate_answer,\n",
    "        )\n",
    "\n",
    "    except Exception as exc:\n",
    "        tb = traceback.format_exc(limit=2)\n",
    "        logs.append(f\"ERROR: {exc}\\n{tb}\")\n",
    "        return (\n",
    "            None,\n",
    "            None,\n",
    "            \"\",\n",
    "            pd.DataFrame(),\n",
    "            None,\n",
    "            \"\\n\".join(logs),\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "        )\n",
    "\n",
    "\n",
    "def regrade_pipeline(rubric_file, grader_model, api_key_text,\n",
    "                     exact_weight, rubric_weight,\n",
    "                     saved_prompt_text, saved_prompt_name, saved_submission_name, saved_candidate_answer,\n",
    "                     override_answer):\n",
    "    logs = []\n",
    "\n",
    "    def log(msg):\n",
    "        logs.append(str(msg))\n",
    "\n",
    "    try:\n",
    "        _ensure_client_from_key(api_key_text)\n",
    "        log(\"OpenAI client ready (regrade).\")\n",
    "\n",
    "        if not rubric_file:\n",
    "            return (\n",
    "                None,\n",
    "                None,\n",
    "                saved_candidate_answer or \"\",\n",
    "                pd.DataFrame(),\n",
    "                None,\n",
    "                \"\\n\".join(logs + [\"Please upload a rubric JSON file to regrade.\"]),\n",
    "                saved_prompt_text,\n",
    "                saved_prompt_name,\n",
    "                saved_submission_name,\n",
    "                saved_candidate_answer,\n",
    "            )\n",
    "\n",
    "        candidate_answer = override_answer.strip() if override_answer and override_answer.strip() else saved_candidate_answer\n",
    "        if not candidate_answer:\n",
    "            return (\n",
    "                None,\n",
    "                None,\n",
    "                \"\",\n",
    "                pd.DataFrame(),\n",
    "                None,\n",
    "                \"\\n\".join(logs + [\"No candidate response available to grade. Upload a final state or paste an override.\"]),\n",
    "                saved_prompt_text,\n",
    "                saved_prompt_name,\n",
    "                saved_submission_name,\n",
    "                saved_candidate_answer,\n",
    "            )\n",
    "\n",
    "        rubric_path = _fpath(rubric_file)\n",
    "        rubric_raw = _read_json_file(rubric_path)\n",
    "        if not isinstance(rubric_raw, list) or not all((\"criterion\" in x and \"score\" in x) for x in rubric_raw):\n",
    "            return (\n",
    "                None,\n",
    "                None,\n",
    "                candidate_answer,\n",
    "                pd.DataFrame(),\n",
    "                None,\n",
    "                \"\\n\".join(logs + [\"Rubric must be a JSON list of {criterion, score} objects.\"]),\n",
    "                saved_prompt_text,\n",
    "                saved_prompt_name,\n",
    "                saved_submission_name,\n",
    "                saved_candidate_answer,\n",
    "            )\n",
    "\n",
    "        rubric_name = Path(rubric_path).name if rubric_path else \"\"\n",
    "\n",
    "        sample = {\"prompt\": saved_prompt_text or \"\", \"reference\": \"\"}\n",
    "        final_norm, details = multigrader(\n",
    "            sample,\n",
    "            candidate_answer,\n",
    "            rubric_raw,\n",
    "            grader_model=grader_model,\n",
    "            exact_weight=float(exact_weight),\n",
    "            rubric_weight=float(rubric_weight),\n",
    "        )\n",
    "        log(\"Regrade complete.\")\n",
    "\n",
    "        items_raw = (details or {}).get(\"rubric_details\", {}).get(\"items\", [])\n",
    "        items_df = _build_items_table(items_raw)\n",
    "        signed_total = float((details or {}).get(\"rubric_total\", 0.0))\n",
    "        final_norm = float(final_norm)\n",
    "\n",
    "        out_dir = _out_dir()\n",
    "        ts = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        pdf_path = out_dir / f\"grading_regrade_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf\"\n",
    "        _make_pdf(pdf_path, {\n",
    "            \"timestamp\": ts,\n",
    "            \"submission_name\": saved_submission_name,\n",
    "            \"grader_model\": grader_model,\n",
    "            \"prompt_name\": saved_prompt_name,\n",
    "            \"rubric_name\": rubric_name,\n",
    "            \"candidate_answer\": candidate_answer,\n",
    "            \"items_df\": items_df,\n",
    "            \"signed_total\": signed_total,\n",
    "            \"normalized\": final_norm,\n",
    "        })\n",
    "        log(f\"Regrade PDF created: {pdf_path}\")\n",
    "\n",
    "        return (\n",
    "            final_norm,\n",
    "            signed_total,\n",
    "            candidate_answer,\n",
    "            items_df,\n",
    "            str(pdf_path),\n",
    "            \"\\n\".join(logs),\n",
    "            saved_prompt_text,\n",
    "            saved_prompt_name,\n",
    "            saved_submission_name,\n",
    "            candidate_answer,\n",
    "        )\n",
    "\n",
    "    except Exception as exc:\n",
    "        tb = traceback.format_exc(limit=2)\n",
    "        logs.append(f\"ERROR: {exc}\\n{tb}\")\n",
    "        return (\n",
    "            None,\n",
    "            None,\n",
    "            saved_candidate_answer or \"\",\n",
    "            pd.DataFrame(),\n",
    "            None,\n",
    "            \"\\n\".join(logs),\n",
    "            saved_prompt_text,\n",
    "            saved_prompt_name,\n",
    "            saved_submission_name,\n",
    "            saved_candidate_answer,\n",
    "        )\n",
    "\n",
    "\n",
    "def _instruction_md():\n",
    "    return (\n",
    "        \"## Rubric Grader\\n\"\n",
    "        \"Upload a rubric (JSON) and a final state export (JSON) produced by your workflow. \"\n",
    "        \"Optionally include the original prompt text so it can be embedded in the grading context. \"\n",
    "        \"Choose a grader model and click **Grade Final State**. You can optionally paste a different response \"\n",
    "        \"and press **Regrade** to compare graders without uploading again.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def build_ui():\n",
    "    default_grader_models = [\n",
    "        \"gpt-4.1\",\n",
    "        \"gpt-4o-mini\",\n",
    "        \"o4-mini-2025-04-16\",\n",
    "    ]\n",
    "\n",
    "    with gr.Blocks(title=\"Rubric Grader\") as demo:\n",
    "        gr.Markdown(_instruction_md())\n",
    "\n",
    "        with gr.Row():\n",
    "            prompt_file = gr.File(label=\"Prompt (.txt/.md, optional)\", file_types=[\".txt\", \".md\"], type=\"filepath\")\n",
    "            rubric_file = gr.File(label=\"Rubric (.json)\", file_types=[\".json\"], type=\"filepath\")\n",
    "            final_state_file = gr.File(label=\"Final state (.json)\", file_types=[\".json\"], type=\"filepath\")\n",
    "\n",
    "        with gr.Row():\n",
    "            grader_model = gr.Dropdown(\n",
    "                default_grader_models,\n",
    "                label=\"Grader model\",\n",
    "                value=default_grader_models[0],\n",
    "                allow_custom_value=True,\n",
    "            )\n",
    "\n",
    "        with gr.Accordion(\"Advanced / Auth\", open=False):\n",
    "            api_key_text = gr.Textbox(label=\"OpenAI API Key (optional if set in env)\", type=\"password\", placeholder=\"sk-...\")\n",
    "            with gr.Row():\n",
    "                exact_weight = gr.Slider(0.0, 1.0, value=0.0, step=0.05, label=\"Exact-match weight\")\n",
    "                rubric_weight = gr.Slider(0.0, 1.0, value=1.0, step=0.05, label=\"Rubric weight\")\n",
    "\n",
    "        with gr.Row():\n",
    "            run_btn = gr.Button(\"Grade Final State\", variant=\"primary\")\n",
    "            regrade_btn = gr.Button(\"Regrade current response\")\n",
    "\n",
    "        with gr.Row():\n",
    "            final_norm = gr.Number(label=\"Final score (normalized 0..1)\")\n",
    "            total_signed = gr.Number(label=\"Total score (signed)\")\n",
    "        sample_response = gr.Textbox(label=\"Candidate response\", lines=12, interactive=False)\n",
    "        items_table_out = gr.Dataframe(label=\"Criterion scores\", interactive=False, wrap=True)\n",
    "        pdf_out = gr.File(label=\"Download report (PDF)\")\n",
    "        logs_out = gr.Textbox(label=\"Run log\", lines=6)\n",
    "\n",
    "        with gr.Accordion(\"Regrade options\", open=False):\n",
    "            override_answer = gr.Textbox(\n",
    "                label=\"Override candidate response (optional)\",\n",
    "                lines=8,\n",
    "                placeholder=\"Paste a response to grade without uploading a new final state.\",\n",
    "            )\n",
    "\n",
    "        saved_prompt_text = gr.State(\"\")\n",
    "        saved_prompt_name = gr.State(\"\")\n",
    "        saved_submission_name = gr.State(\"\")\n",
    "        saved_candidate_answer = gr.State(\"\")\n",
    "\n",
    "        run_btn.click(\n",
    "            fn=run_pipeline,\n",
    "            inputs=[\n",
    "                prompt_file,\n",
    "                rubric_file,\n",
    "                final_state_file,\n",
    "                grader_model,\n",
    "                api_key_text,\n",
    "                exact_weight,\n",
    "                rubric_weight,\n",
    "            ],\n",
    "            outputs=[\n",
    "                final_norm,\n",
    "                total_signed,\n",
    "                sample_response,\n",
    "                items_table_out,\n",
    "                pdf_out,\n",
    "                logs_out,\n",
    "                saved_prompt_text,\n",
    "                saved_prompt_name,\n",
    "                saved_submission_name,\n",
    "                saved_candidate_answer,\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        regrade_btn.click(\n",
    "            fn=regrade_pipeline,\n",
    "            inputs=[\n",
    "                rubric_file,\n",
    "                grader_model,\n",
    "                api_key_text,\n",
    "                exact_weight,\n",
    "                rubric_weight,\n",
    "                saved_prompt_text,\n",
    "                saved_prompt_name,\n",
    "                saved_submission_name,\n",
    "                saved_candidate_answer,\n",
    "                override_answer,\n",
    "            ],\n",
    "            outputs=[\n",
    "                final_norm,\n",
    "                total_signed,\n",
    "                sample_response,\n",
    "                items_table_out,\n",
    "                pdf_out,\n",
    "                logs_out,\n",
    "                saved_prompt_text,\n",
    "                saved_prompt_name,\n",
    "                saved_submission_name,\n",
    "                saved_candidate_answer,\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    return demo\n",
    "\n",
    "\n",
    "demo = build_ui()\n",
    "demo.launch(share=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df803a70-f0aa-48ca-bb72-e5d2351f2448",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}